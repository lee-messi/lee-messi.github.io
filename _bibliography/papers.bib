---

@misc{jeon_demographic_2025,
  title = {Demographic {Biases} in {Political} {Ideology} {Attribution} by {Vision}-{Language} {Models}},
  author = {Jeon, Soyeon and Lee, Messi H.J. and Montgomery, Jacob M. and Lai, Calvin K.},
  year = {2025},
  month = sep,
  primaryclass = {cs},
  abstract = {When foundation models analyze political content, do they use demographic characteristics as shortcuts for ideological attribution? We conducted detailed experiments with GPT-4o-mini and validated key findings across GPT-4o and LLaVA, using identical, ideologically neutral campaign advertisements with systematically varied candidate demographics. All models consistently attributed more liberal ideologies to women than men. These effects exceeded real-world gender differences from a nationally representative survey. However, racial associations differed by model: strong in GPT-4o-mini (where Black candidates received substantially more liberal attributions), attenuated in \texttt{GPT-4o}, and insignificant in LLaVA. These demographic effects persisted across temperature settings, prompt variations, and even explicit debiasing instructions in GPT-4o-mini. Our findings reveal that visual demographic features can shape AI outputs in ways that vary across models, with implications for applications ranging from content classification to simulated voter responses.},
  keywords = {Political Science,Computer Science - Computers and Society},
  selected = {false}
}


@misc{lee_implicit_2025,
  title = {Implicit {{Bias-Like Patterns}} in {{Reasoning Models}}},
  author = {Lee, Messi H. J. and Lai, Calvin K.},
  year = {2025},
  month = mar,
  number = {arXiv:2503.11572},
  eprint = {2503.11572},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.11572},
  urldate = {2025-03-17},
  abstract = {Implicit bias refers to automatic or spontaneous mental processes that shape perceptions, judgments, and behaviors. Previous research examining `implicit bias' in large language models (LLMs) has often approached the phenomenon differently than how it is studied in humans by focusing primarily on model outputs rather than on model processing. To examine model processing, we present a method called the Reasoning Model Implicit Association Test (RM-IAT) for studying implicit bias-like patterns in reasoning models: LLMs that employ step-by-step reasoning to solve complex tasks. Using this method, we find that reasoning models require more tokens when processing association-incompatible information compared to association-compatible information. These findings suggest AI systems harbor patterns in processing information that are analogous to human implicit bias. We consider the implications of these implicit bias-like patterns for their deployment in real-world applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society},
  selected = {true},
  pdf = {lee_implicit_2025.pdf}
}


@misc{lee_visual_2025,
  title = {Visual {{Cues}} of {{Gender}} and {{Race}} Are {{Associated}} with {{Stereotyping}} in {{Vision-Language Models}}},
  author = {Lee, Messi H. J. and Jeon, Soyeon and Montgomery, Jacob M. and Lai, Calvin K.},
  year = {2025},
  month = mar,
  number = {arXiv:2503.05093},
  eprint = {2503.05093},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.05093},
  urldate = {2025-03-14},
  abstract = {Current research on bias in Vision Language Models (VLMs) has important limitations: it is focused exclusively on trait associations while ignoring other forms of stereotyping, it examines specific contexts where biases are expected to appear, and it conceptualizes social categories like race and gender as binary, ignoring the multifaceted nature of these identities. Using standardized facial images that vary in prototypicality, we test four VLMs for both trait associations and homogeneity bias in open-ended contexts. We find that VLMs consistently generate more uniform stories for women compared to men, with people who are more gender prototypical in appearance being represented more uniformly. By contrast, VLMs represent White Americans more uniformly than Black Americans. Unlike with gender prototypicality, race prototypicality was not related to stronger uniformity. In terms of trait associations, we find limited evidence of stereotyping-Black Americans were consistently linked with basketball across all models, while other racial associations (i.e., art, healthcare, appearance) varied by specific VLM. These findings demonstrate that VLM stereotyping manifests in ways that go beyond simple group membership, suggesting that conventional bias mitigation strategies may be insufficient to address VLM stereotyping and that homogeneity bias persists even when trait associations are less apparent in model outputs.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  selected = {false},
  pdf = {lee_visual_2025.pdf}
}


@misc{lee_homogeneity_2025,
  title = {Homogeneity {{Bias}} as {{Differential Sampling Uncertainty}} in {{Language Models}}},
  author = {Lee, Messi H. J. and Jeon, Soyeon},
  year = {2025},
  month = jan,
  number = {arXiv:2501.19337},
  eprint = {2501.19337},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.19337},
  urldate = {2025-02-03},
  abstract = {Prior research show that Large Language Models (LLMs) and Vision-Language Models (VLMs) represent marginalized groups more homogeneously than dominant groups. However, the mechanisms underlying this homogeneity bias remain relatively unexplored. We propose that this bias emerges from systematic differences in the probability distributions from which tokens are sampled at inference-time. Analyzing three measures of uncertainty in token sampling distributions-entropy, perplexity, and probability of differentiation-we find that in some models, specifically GPT-4 Turbo and Llama-3.2, tokens are sampled more deterministically when generating texts about marginalized groups (i.e., Black Americans and women) compared to their dominant group counterparts (i.e., White Americans and men). While these findings may help explain homogeneity bias in certain models, the patterns did not replicate across all VLMs tested, suggesting multiple mechanisms may contribute to homogeneity bias in AI.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  selected = {false},
  pdf = {lee_homogeneity_2025.pdf}
}


@misc{lee_examining_2025,
  title = {Examining the {{Robustness}} of {{Homogeneity Bias}} to {{Hyperparameter Adjustments}} in {{GPT-4}}},
  author = {Lee, Messi H. J.},
  year = {2025},
  month = jan,
  number = {arXiv:2501.02211},
  eprint = {2501.02211},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.02211},
  urldate = {2025-01-07},
  abstract = {Vision-Language Models trained on massive collections of human-generated data often reproduce and amplify societal stereotypes. One critical form of stereotyping reproduced by these models is homogeneity bias-the tendency to represent certain groups as more homogeneous than others. We investigate how this bias responds to hyperparameter adjustments in GPT-4, specifically examining sampling temperature and top p which control the randomness of model outputs. By generating stories about individuals from different racial and gender groups and comparing their similarities using vector representations, we assess both bias robustness and its relationship with hyperparameter values. We find that (1) homogeneity bias persists across most hyperparameter configurations, with Black Americans and women being represented more homogeneously than White Americans and men, (2) the relationship between hyperparameters and group representations shows unexpected non-linear patterns, particularly at extreme values, and (3) hyperparameter adjustments affect racial and gender homogeneity bias differently-while increasing temperature or decreasing top p can reduce racial homogeneity bias, these changes show different effects on gender homogeneity bias. Our findings suggest that while hyperparameter tuning may mitigate certain biases to some extent, it cannot serve as a universal solution for addressing homogeneity bias across different social group dimensions.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  selected = {false},
  pdf = {lee_hyperparameters_2025.pdf}
}


@misc{lee_visionlanguage_2025,
  title = {Vision-{{Language Models Generate More Homogeneous Stories}} for {{Phenotypically Black Individuals}}},
  author = {Lee, Messi H. J. and Jeon, Soyeon},
  year = {2025},
  month = mar,
  number = {arXiv:2412.09668},
  eprint = {2412.09668},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.09668},
  urldate = {2025-03-21},
  abstract = {Vision-Language Models (VLMs) extend Large Language Models' capabilities by integrating image processing, but concerns persist about their potential to reproduce and amplify human biases. While research has documented how these models perpetuate stereotypes across demographic groups, most work has focused on between-group biases rather than within-group differences. This study investigates homogeneity bias-the tendency to portray groups as more uniform than they are-within Black Americans, examining how perceived racial phenotypicality influences VLMs' outputs. Using computer-generated images that systematically vary in phenotypicality, we prompted VLMs to generate stories about these individuals and measured text similarity to assess content homogeneity. Our findings reveal three key patterns: First, VLMs generate significantly more homogeneous stories about Black individuals with higher phenotypicality compared to those with lower phenotypicality. Second, stories about Black women consistently display greater homogeneity than those about Black men across all models tested. Third, in two of three VLMs, this homogeneity bias is primarily driven by a pronounced interaction where phenotypicality strongly influences content variation for Black women but has minimal impact for Black men. These results demonstrate how intersectionality shapes AI-generated representations and highlight the persistence of stereotyping that mirror documented biases in human perception, where increased racial phenotypicality leads to greater stereotyping and less individualized representation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  selected = {true},
  pdf = {lee_visionlanguage_2025.pdf}
}


@misc{lee_probability_2024,
  title = {Probability of {{Differentiation Reveals Brittleness}} of {{Homogeneity Bias}} in {{Large Language Models}}},
  author = {Lee, Messi H. J. and Lai, Calvin K.},
  year = {2024},
  month = jul,
  number = {arXiv:2407.07329},
  eprint = {2407.07329},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-11},
  abstract = {Homogeneity bias in Large Language Models (LLMs) refers to their tendency to homogenize the representations of some groups compared to others. Previous studies documenting this bias have predominantly used encoder models, which may have inadvertently introduced biases. To address this limitation, we prompted GPT-4 to generate single word/expression completions associated with 18 situation cues-specific, measurable elements of environments that influence how individuals perceive situations and compared the variability of these completions using probability of differentiation. This approach directly assessed homogeneity bias from the model's outputs, bypassing encoder models. Across five studies, we find that homogeneity bias is highly volatile across situation cues and writing prompts, suggesting that the bias observed in past work may reflect those within encoder models rather than LLMs. Furthermore, we find that homogeneity bias in LLMs is brittle, as even minor and arbitrary changes in prompts can significantly alter the expression of biases. Future work should further explore how variations in syntactic features and topic choices in longer text generations influence homogeneity bias in LLMs.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  selected = {false},
  pdf = {lee_probability_2024.pdf}
}


@inproceedings{lee_large_2024,
  title = {Large {{Language Models Portray Socially Subordinate Groups}} as {{More Homogeneous}}, {{Consistent}} with a {{Bias Observed}} in {{Humans}}},
  booktitle = {The 2024 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Lee, Messi H.J. and Montgomery, Jacob M. and Lai, Calvin K.},
  year = {2024},
  month = jun,
  series = {{{FAccT}} '24},
  pages = {1321--1340},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3630106.3658975},
  urldate = {2024-06-05},
  abstract = {Large language models (LLMs) are becoming pervasive in everyday life, yet their propensity to reproduce biases inherited from training data remains a pressing concern. Prior investigations into bias in LLMs have focused on the association of social groups with stereotypical attributes. However, this is only one form of human bias such systems may reproduce. We investigate a new form of bias in LLMs that resembles a social psychological phenomenon where socially subordinate groups are perceived as more homogeneous than socially dominant groups. We had ChatGPT, a state-of-the-art LLM, generate texts about intersectional group identities and compared those texts on measures of homogeneity. We consistently found that ChatGPT portrayed African, Asian, and Hispanic Americans as more homogeneous than White Americans, indicating that the model described racial minority groups with a narrower range of human experience. ChatGPT also portrayed women as more homogeneous than men, but these differences were small. Finally, we found that the effect of gender differed across racial/ethnic groups such that the effect of gender was consistent within African and Hispanic Americans but not within Asian and White Americans. We argue that the tendency of LLMs to describe groups as less diverse risks perpetuating stereotypes and discriminatory behavior.},
  copyright = {All rights reserved},
  isbn = {9798400704505},
  keywords = {AI Bias,Homogeneity Bias,Large Language Models,Perceived Variability,Stereotyping},
  selected = {true},
  pdf = {lee_large_2024.pdf}
}


@article{lee_americas_2024,
  title = {America's Racial Framework of Superiority and {{Americanness}} Embedded in Natural Language},
  author = {Lee, Messi H. J. and Montgomery, Jacob M. and Lai, Calvin K.},
  year = {2024},
  month = jan,
  journal = {PNAS Nexus},
  volume = {3},
  number = {1},
  pages = {pgad485},
  issn = {2752-6542},
  doi = {10.1093/pnasnexus/pgad485},
  url = {https://academic.oup.com/pnasnexus/article/3/1/pgad485/7504812}, 
  urldate = {2024-01-27},
  abstract = {America's racial framework can be summarized using two distinct dimensions: superiority/inferiority and Americanness/foreignness. We investigated America's racial framework in a corpus of spoken and written language using word embeddings. Word embeddings place words on a low-dimensional space where words with similar meanings are proximate, allowing researchers to test whether the positions of group and attribute words in a semantic space reflect stereotypes. We trained a word embedding model on the Corpus of Contemporary American English{\textemdash}a corpus of 1 billion words that span 30 years and 8 text categories{\textemdash}and compared the positions of racial/ethnic groups with respect to superiority and Americanness. We found that America's racial framework is embedded in American English. We also captured an additional nuance: Asian people were stereotyped as more American than Hispanic people. These results are empirical evidence that America's racial framework is embedded in American English.},
  copyright = {All rights reserved},
  selected = {false},
  pdf = {lee_americas_2024.pdf}
}


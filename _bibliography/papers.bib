---

@misc{lee_examining_2025,
  title = {Examining the {{Robustness}} of {{Homogeneity Bias}} to {{Hyperparameter Adjustments}} in {{GPT-4}}},
  author = {Lee, Messi H. J.},
  year = {2025},
  month = jan,
  number = {arXiv:2501.02211},
  eprint = {2501.02211},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.02211},
  urldate = {2025-01-07},
  abstract = {Vision-Language Models trained on massive collections of human-generated data often reproduce and amplify societal stereotypes. One critical form of stereotyping reproduced by these models is homogeneity bias-the tendency to represent certain groups as more homogeneous than others. We investigate how this bias responds to hyperparameter adjustments in GPT-4, specifically examining sampling temperature and top p which control the randomness of model outputs. By generating stories about individuals from different racial and gender groups and comparing their similarities using vector representations, we assess both bias robustness and its relationship with hyperparameter values. We find that (1) homogeneity bias persists across most hyperparameter configurations, with Black Americans and women being represented more homogeneously than White Americans and men, (2) the relationship between hyperparameters and group representations shows unexpected non-linear patterns, particularly at extreme values, and (3) hyperparameter adjustments affect racial and gender homogeneity bias differently-while increasing temperature or decreasing top p can reduce racial homogeneity bias, these changes show different effects on gender homogeneity bias. Our findings suggest that while hyperparameter tuning may mitigate certain biases to some extent, it cannot serve as a universal solution for addressing homogeneity bias across different social group dimensions.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  selected = {true},
  pdf = {lee_hyperparameters_2025.pdf}
}


@misc{lee_visionlanguage_2024,
  title = {Vision-{{Language Models Represent Darker-Skinned Black Individuals}} as {{More Homogeneous}} than {{Lighter-Skinned Black Individuals}}},
  author = {Lee, Messi H. J. and Jeon, Soyeon},
  year = {2024},
  month = dec,
  number = {arXiv:2412.09668},
  eprint = {2412.09668},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.09668},
  urldate = {2024-12-16},
  abstract = {Vision-Language Models (VLMs) combine Large Language Model (LLM) capabilities with image processing, enabling tasks like image captioning and text-to-image generation. Yet concerns persist about their potential to amplify human-like biases, including skin tone bias. Skin tone bias, where darker-skinned individuals face more negative stereotyping than lighter-skinned individuals, is well-documented in the social sciences but remains under-explored in Artificial Intelligence (AI), particularly in VLMs. While well-documented in the social sciences, this bias remains under-explored in AI, particularly in VLMs. Using the GAN Face Database, we sampled computer-generated images of Black American men and women, controlling for skin tone variations while keeping other features constant. We then asked VLMs to write stories about these faces and compared the homogeneity of the generated stories. Stories generated by VLMs about darker-skinned Black individuals were more homogeneous than those about lighter-skinned individuals in three of four models, and Black women were consistently represented more homogeneously than Black men across all models. Interaction effects revealed a greater impact of skin tone on women in two VLMs, while the other two showed nonsignificant results, reflecting known stereotyping patterns. These findings underscore the propagation of biases from single-modality AI systems to multimodal models and highlight the need for further research to address intersectional biases in AI.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  selected = {true},
  pdf = {lee_visionlanguage_2024.pdf}
}


@misc{lee_probability_2024,
  title = {Probability of {{Differentiation Reveals Brittleness}} of {{Homogeneity Bias}} in {{Large Language Models}}},
  author = {Lee, Messi H. J. and Lai, Calvin K.},
  year = {2024},
  month = jul,
  number = {arXiv:2407.07329},
  eprint = {2407.07329},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-11},
  abstract = {Homogeneity bias in Large Language Models (LLMs) refers to their tendency to homogenize the representations of some groups compared to others. Previous studies documenting this bias have predominantly used encoder models, which may have inadvertently introduced biases. To address this limitation, we prompted GPT-4 to generate single word/expression completions associated with 18 situation cues-specific, measurable elements of environments that influence how individuals perceive situations and compared the variability of these completions using probability of differentiation. This approach directly assessed homogeneity bias from the model's outputs, bypassing encoder models. Across five studies, we find that homogeneity bias is highly volatile across situation cues and writing prompts, suggesting that the bias observed in past work may reflect those within encoder models rather than LLMs. Furthermore, we find that homogeneity bias in LLMs is brittle, as even minor and arbitrary changes in prompts can significantly alter the expression of biases. Future work should further explore how variations in syntactic features and topic choices in longer text generations influence homogeneity bias in LLMs.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  selected = {true},
  pdf = {lee_probability_2024.pdf}
}


@misc{lee_more_2024,
  title = {More {{Distinctively Black}} and {{Feminine Faces Lead}} to {{Increased Stereotyping}} in {{Vision-Language Models}}},
  author = {Lee, Messi H. J. and Montgomery, Jacob M. and Lai, Calvin K.},
  year = {2024},
  month = may,
  number = {arXiv:2407.06194},
  eprint = {2407.06194},
  publisher = {arXiv},
  urldate = {2024-07-10},
  abstract = {Vision Language Models (VLMs), exemplified by GPT-4V, adeptly integrate text and vision modalities. This integration enhances Large Language Models' ability to mimic human perception, allowing them to process image inputs. Despite VLMs' advanced capabilities, however, there is a concern that VLMs inherit biases of both modalities in ways that make biases more pervasive and difficult to mitigate. Our study explores how VLMs perpetuate homogeneity bias and trait associations with regards to race and gender. When prompted to write stories based on images of human faces, GPT-4V describes subordinate racial and gender groups with greater homogeneity than dominant groups and relies on distinct, yet generally positive, stereotypes. Importantly, VLM stereotyping is driven by visual cues rather than group membership alone such that faces that are rated as more prototypically Black and feminine are subject to greater stereotyping. These findings suggest that VLMs may associate subtle visual cues related to racial and gender groups with stereotypes in ways that could be challenging to mitigate. We explore the underlying reasons behind this behavior and discuss its implications and emphasize the importance of addressing these biases as VLMs come to mirror human perception.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  selected = {false}
}


@inproceedings{lee_large_2024,
  title = {Large {{Language Models Portray Socially Subordinate Groups}} as {{More Homogeneous}}, {{Consistent}} with a {{Bias Observed}} in {{Humans}}},
  booktitle = {The 2024 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Lee, Messi H.J. and Montgomery, Jacob M. and Lai, Calvin K.},
  year = {2024},
  month = jun,
  series = {{{FAccT}} '24},
  pages = {1321--1340},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3630106.3658975},
  urldate = {2024-06-05},
  abstract = {Large language models (LLMs) are becoming pervasive in everyday life, yet their propensity to reproduce biases inherited from training data remains a pressing concern. Prior investigations into bias in LLMs have focused on the association of social groups with stereotypical attributes. However, this is only one form of human bias such systems may reproduce. We investigate a new form of bias in LLMs that resembles a social psychological phenomenon where socially subordinate groups are perceived as more homogeneous than socially dominant groups. We had ChatGPT, a state-of-the-art LLM, generate texts about intersectional group identities and compared those texts on measures of homogeneity. We consistently found that ChatGPT portrayed African, Asian, and Hispanic Americans as more homogeneous than White Americans, indicating that the model described racial minority groups with a narrower range of human experience. ChatGPT also portrayed women as more homogeneous than men, but these differences were small. Finally, we found that the effect of gender differed across racial/ethnic groups such that the effect of gender was consistent within African and Hispanic Americans but not within Asian and White Americans. We argue that the tendency of LLMs to describe groups as less diverse risks perpetuating stereotypes and discriminatory behavior.},
  copyright = {All rights reserved},
  isbn = {9798400704505},
  keywords = {AI Bias,Homogeneity Bias,Large Language Models,Perceived Variability,Stereotyping},
  selected = {true},
  pdf = {lee_large_2024.pdf}
}


@article{lee_americas_2024,
  title = {America's Racial Framework of Superiority and {{Americanness}} Embedded in Natural Language},
  author = {Lee, Messi H. J. and Montgomery, Jacob M. and Lai, Calvin K.},
  year = {2024},
  month = jan,
  journal = {PNAS Nexus},
  volume = {3},
  number = {1},
  pages = {pgad485},
  issn = {2752-6542},
  doi = {10.1093/pnasnexus/pgad485},
  url = {https://academic.oup.com/pnasnexus/article/3/1/pgad485/7504812}, 
  urldate = {2024-01-27},
  abstract = {America's racial framework can be summarized using two distinct dimensions: superiority/inferiority and Americanness/foreignness. We investigated America's racial framework in a corpus of spoken and written language using word embeddings. Word embeddings place words on a low-dimensional space where words with similar meanings are proximate, allowing researchers to test whether the positions of group and attribute words in a semantic space reflect stereotypes. We trained a word embedding model on the Corpus of Contemporary American English{\textemdash}a corpus of 1 billion words that span 30 years and 8 text categories{\textemdash}and compared the positions of racial/ethnic groups with respect to superiority and Americanness. We found that America's racial framework is embedded in American English. We also captured an additional nuance: Asian people were stereotyped as more American than Hispanic people. These results are empirical evidence that America's racial framework is embedded in American English.},
  copyright = {All rights reserved},
  selected = {true},
  pdf = {lee_americas_2024.pdf}
}


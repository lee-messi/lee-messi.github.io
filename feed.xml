<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://lee-messi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lee-messi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-22T00:09:11+00:00</updated><id>https://lee-messi.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Homogeneity Bias as Differential Sampling Uncertainty</title><link href="https://lee-messi.github.io/blog/2025/sampling-uncertainty/" rel="alternate" type="text/html" title="Homogeneity Bias as Differential Sampling Uncertainty"/><published>2025-02-04T00:00:00+00:00</published><updated>2025-02-04T00:00:00+00:00</updated><id>https://lee-messi.github.io/blog/2025/sampling-uncertainty</id><content type="html" xml:base="https://lee-messi.github.io/blog/2025/sampling-uncertainty/"><![CDATA[<p>Recently, I came across an interesting paper investigating how regularizing attention weights could encourage BERT-like models to consider broader token contexts when constructing token-level representations thereby mitigating social biases (Attanasio et al., 2022). That’s quite technical, so let me walk you through what it means in simpler terms.</p> <h2 id="the-connection-between-attention-and-bias">The Connection between Attention and Bias</h2> <p>Think of attention in NLP models like a spotlight. When a model processes a word (token), it shines this spotlight on other words in the context to understand its meaning. For example, when processing the word “doctor,” traditional models might shine a very bright spotlight on words like “he” and a dimmer one on “she” - because that’s the pattern they learned from training data.</p> <p>The key idea behind attention regularization is to intentionally diffuse this spotlight–instead of having intense focus on specific gender-associated words, we want the model to distribute its attention more evenly. Here’s a way to think about it:</p> <p>In a normal setup, when processing “doctor”, the attention weights might look like:</p> <ul> <li>“he”: 0.7</li> <li>“she”: 0.2</li> <li>non-gendered pronouns: 0.1</li> </ul> <p>With regularization, we push for more balanced attention:</p> <ul> <li>“he”: 0.4</li> <li>“she”: 0.4</li> <li>non-gendered pronouns: 0.2</li> </ul> <p>By enforcing this more uniform distribution of attention, we reduce the model’s tendency to strongly associate certain professions with specific genders. It’s like telling the model: “Don’t jump to conclusions about gender when you’re thinking about occupations.”</p> <h2 id="how-it-relates-to-homogeneity-bias">How it Relates to Homogeneity Bias</h2> <p>This work on regularizing attention weights to mitigate social bias led me to consider homogeneity bias from a new angle. What if homogeneity bias in language models manifests from how these models sample tokens during inference? Specifically, when generating text about marginalized groups, the token sampling distribution might become more deterministic, focusing on a narrower set of possible next words.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-04-sampling-uncertainty/study_design-480.webp 480w,/assets/img/2025-02-04-sampling-uncertainty/study_design-800.webp 800w,/assets/img/2025-02-04-sampling-uncertainty/study_design-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-04-sampling-uncertainty/study_design.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We tested this idea using Vision-Language Models as our experimental setup. We asked these models to generate stories about images showing different racial and gender groups, then carefully tracked how the models made their word choices. To measure this scientifically, we used three different metrics: entropy, perplexity, and probability of differentiation. These metrics helped us understand if the model was more predictable when writing about certain groups versus others-similar to how a recommendation system might fall back on popular, safe suggestions when it has limited data about a user’s preferences.</p> <h2 id="results">Results</h2> <p>Our initial analysis with GPT-4 Turbo revealed a consistent pattern: when generating text, the model showed significantly lower uncertainty in its word choices when writing about Black Americans and women compared to White Americans and men. This pattern held true across all 50 token positions we analyzed.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-04-sampling-uncertainty/race-480.webp 480w,/assets/img/2025-02-04-sampling-uncertainty/race-800.webp 800w,/assets/img/2025-02-04-sampling-uncertainty/race-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-04-sampling-uncertainty/race.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To verify whether this pattern was unique to GPT-4 Turbo, we expanded our analysis to include three additional models: GPT-4o mini, Llama-3.2, and Ovis-1.6. Llama-3.2 exhibited similar patterns to GPT-4 Turbo, while GPT-4o mini and Ovis-1.6 either showed no significant differences or displayed opposite patterns in their token sampling behavior.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-04-sampling-uncertainty/race_position-480.webp 480w,/assets/img/2025-02-04-sampling-uncertainty/race_position-800.webp 800w,/assets/img/2025-02-04-sampling-uncertainty/race_position-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-04-sampling-uncertainty/race_position.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The visualizations above demonstrate consistently lower measures across token positions for Black Americans compared to White Americans, indicating more deterministic token sampling distributions for Black Americans.</p> <h2 id="the-significance-of-this-work">The Significance of this Work</h2> <p>Our approach offers a new way to understand homogeneity bias in language models by looking directly at how these models make word choices during text generation. Previous methods relied on using encoder models to analyze the final generated text, but these methods shared a fatal limitation: these encoder models might share the same biases as the models being studied, making it harder to isolate different sources of bias.</p> <p>By examining token sampling distributions during inference, we get a more direct window into how these models actually behave when generating text. Rather than looking at the end product, we’re observing the model’s decision-making process in real-time – specifically, how certain or uncertain it is when choosing words for different groups. This mechanistic approach helps us pinpoint where and how homogeneity bias manifests in the generation process, opening new possibilities for addressing these biases at their source.</p> <h3 id="references">References</h3> <p>Lee, M. H. J., &amp; Jeon, S. (2025). Homogeneity Bias as Differential Sampling Uncertainty in Language Models (arXiv:2501.19337). arXiv. https://doi.org/10.48550/arXiv.2501.19337</p> <p>Attanasio, G., Nozza, D., Hovy, D., &amp; Baralis, E. (2022). Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists (arXiv:2203.09192). arXiv. https://doi.org/10.48550/arXiv.2203.09192</p>]]></content><author><name></name></author><category term="research"/><summary type="html"><![CDATA[Introduction to a new paper]]></summary></entry><entry><title type="html">Homogeneity Bias in Large Language Models</title><link href="https://lee-messi.github.io/blog/2025/homogeneity-bias/" rel="alternate" type="text/html" title="Homogeneity Bias in Large Language Models"/><published>2025-01-08T00:00:00+00:00</published><updated>2025-01-08T00:00:00+00:00</updated><id>https://lee-messi.github.io/blog/2025/homogeneity-bias</id><content type="html" xml:base="https://lee-messi.github.io/blog/2025/homogeneity-bias/"><![CDATA[<p>This blog post is about my research at the intersection of Social Psychology and Artificial Intelligence. While AI might be familiar to many through recent developments like ChatGPT, the social psychology aspects might be less known. Let me walk you through how these fields connect, starting with a simple classroom example.</p> <h3 id="understanding-social-psychology-through-a-classroom">Understanding Social Psychology Through a Classroom</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-08-homogeneity-bias/classroom-480.webp 480w,/assets/img/2025-01-08-homogeneity-bias/classroom-800.webp 800w,/assets/img/2025-01-08-homogeneity-bias/classroom-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-08-homogeneity-bias/classroom.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Imagine walking into a classroom of 20 students on the first day of school. You’re surrounded by unfamiliar faces, feeling slightly awkward and anxious. As the semester progresses, you work on group projects and play together during recess. Gradually, that initial awkwardness fades as you form friendships. This transformation - how social interactions shape our thoughts, feelings, and behaviors - is what social psychologists study.</p> <h4 id="how-groups-form-and-divide">How Groups Form and Divide</h4> <p>In this classroom, students naturally split into groups. Let’s say one group loves sports and always plays outside during recess, while another prefers board games indoors. This division, though based simply on interests, can grow stronger over time.</p> <p>Think about being in the board game group. At first, you might casually invite someone from the sports group to join your game. After several rejections, you start assuming all sports group members wouldn’t be interested in board games and decide never to invite anyone again from the sports group. This assumption about an entire group is what we call a <em>stereotype</em>. The uncertainty you feel about asking them again is <em>prejudice</em>, and eventually choosing not to invite them is a form of <em>discrimination</em>.</p> <h4 id="seeing-other-groups-as-all-the-same">Seeing Other Groups as “All the Same”</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-08-homogeneity-bias/invitation-480.webp 480w,/assets/img/2025-01-08-homogeneity-bias/invitation-800.webp 800w,/assets/img/2025-01-08-homogeneity-bias/invitation-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-08-homogeneity-bias/invitation.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This classroom dynamic reveals a fascinating pattern: we tend to see members of other groups (outgroups) as more similar to each other than members of our own group (ingroup). As a board game enthusiast, you might think “all the sports kids are alike,” while seeing distinct personalities in your fellow board game players.</p> <h4 id="why-this-matters">Why This Matters</h4> <p>When we start seeing a group as “all the same,” something concerning happens. Research shows that this perception leads to more stereotyping, increased negative prejudice, and greater discrimination. Going back to our classroom example, once you start thinking of the sports group as all alike, you might be less likely to notice their individual interests or invite them to join different activities. You might even start making broader assumptions about what they like or don’t like, further widening the divide between groups.</p> <h3 id="ai-systems-learn-from-humans">AI Systems Learn From Humans</h3> <p>One of the most fascinating developments during my PhD has been the rise of Artificial Intelligence, particularly Large Language Models like ChatGPT. These models have shown an interesting tendency: they reproduce patterns of human stereotyping. For example, when writing stories about different groups, they often introduce the same gender stereotypes that humans have such as associating men with sports and politics, and women with family and emotions.</p> <p>This observation led me to a crucial question: if AI systems mirror human patterns of stereotyping, might they also show our tendency to see certain groups as “all the same”? Just as students in our classroom example might view the sports group as more similar to each other than they really are, would AI systems represent some groups as more similar than others?</p> <h4 id="measuring-ai-bias">Measuring AI Bias</h4> <p>To answer this question, we conducted an experiment. We asked ChatGPT to write various kinds of texts - stories, biographies, and self-introductions - about different racial/ethnic and gender groups. Then came the challenging part: how do we measure if AI sees certain groups as “more similar”?</p> <p>We approached this by examining how similar the AI’s writings were for each group. Think of it like this: if you asked someone to write multiple stories about different board game club members, and the stories were all very similar, you might conclude that this person sees board game club members as “all the same.” We did something similar with AI, using computational tools to measure how similar its writings were for different groups.</p> <h4 id="what-we-found">What We Found</h4> <p>The results were clear and concerning. Just like humans view certain groups as more similar to each other, AI systems showed the same pattern. When writing about racial/ethnic minorities and women, the AI’s stories were more similar to each other compared to its stories about White Americans and men. This happened even when we specifically asked for diverse, non-stereotypical stories.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-08-homogeneity-bias/bias-480.webp 480w,/assets/img/2025-01-08-homogeneity-bias/bias-800.webp 800w,/assets/img/2025-01-08-homogeneity-bias/bias-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-08-homogeneity-bias/bias.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Could this simply be because the AI was using similar topics or themes for certain groups? We tested this possibility too. While we did find that AI tended to write about specific themes (like adversity and hardship) more often for some groups, the similarity in writing went beyond just shared topics. Even when writing about the same theme, the AI’s descriptions were more similar for certain groups than others.</p> <h4 id="why-this-happens">Why This Happens</h4> <p>The explanation likely comes from how AI systems learn. These models are trained on massive amounts of human-written text from the internet and other sources. If human writers tend to represent certain groups in more homogeneous ways, the AI learns and reproduces these patterns. It’s similar to how students might form overly simplified views of other groups based on limited interactions - AI systems form their “views” based on the limited and potentially biased information in their training data.</p> <h4 id="looking-forward">Looking Forward</h4> <p>Our research has recently expanded to study whether these patterns appear when AI systems process facial images. This is particularly important as AI systems become more integrated into our daily lives, from content generation to decision support. Understanding these biases is the first step toward building fairer technology that respects the diversity of groups.</p> <p>If you’d like to learn more about this research, you can read our paper “<a href="https://dl.acm.org/doi/10.1145/3630106.3658975">Large Language Models Portray Socially Subordinate Groups as More Homogeneous, Consistent with a Bias Observed in Humans</a>” published in the ACM Conference on Fairness, Accountability, and Transparency 2024 (FAccT ‘24).</p> <p><em>Note: All visualizations included in the blog post were created using ChatGPT (GPT-4o)</em></p>]]></content><author><name></name></author><category term="research"/><summary type="html"><![CDATA[A short blog post about my research on homogeneity bias in Large Language Models]]></summary></entry></feed>
<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://lee-messi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lee-messi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-10T20:16:59+00:00</updated><id>https://lee-messi.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Implicit Bias-like Patterns in Reasoning Models</title><link href="https://lee-messi.github.io/blog/2025/implicit-bias/" rel="alternate" type="text/html" title="Implicit Bias-like Patterns in Reasoning Models"/><published>2025-03-23T00:00:00+00:00</published><updated>2025-03-23T00:00:00+00:00</updated><id>https://lee-messi.github.io/blog/2025/implicit-bias</id><content type="html" xml:base="https://lee-messi.github.io/blog/2025/implicit-bias/"><![CDATA[<p>Think about a traffic light. When you see a red light, what’s your immediate response? <em>Stop</em>. When you see a green light? <em>Go</em>. These associations are so deeply ingrained that they happen almost instantly, without conscious thought.</p> <p>Now imagine if traffic rules suddenly reversed: green means stop, and red means go. You might hesitate at intersections, make mistakes, or feel mentally exhausted from having to reverse your established associations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-23-implicit-bias/red_light-480.webp 480w,/assets/img/2025-03-23-implicit-bias/red_light-800.webp 800w,/assets/img/2025-03-23-implicit-bias/red_light-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-23-implicit-bias/red_light.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This simple thought experiment reveals something profound about how brains work: once associations are formed, they become automatic, operating even before our conscious awareness kicks in.</p> <h3 id="from-traffic-lights-to-implicit-bias">From Traffic Lights to Implicit Bias</h3> <p>This same automatic processing underlies implicit bias. Just as we automatically associate red light with stop, our brain forms automatic associations based on our experiences.</p> <p>These associations aren’t necessarily malicious–they’re the brain’s attempt to efficiently process the overwhelming amount of information we encounter daily. But efficiency comes at a cost. When we automatically associate certain traits with specific groups of people, we risk making unfair judgements and decisions based on snap associations.</p> <h3 id="measuring-implicit-bias-the-implicit-association-test-iat">Measuring Implicit Bias: The Implicit Association Test (IAT)</h3> <p>This brings us to the Implicit Association Test (IAT). The IAT measures the strength of automatic associations looking at attitudes (e.g., Young/Old People + Pleasant/Unpleasant) or stereotypes (e.g., Male/Female + Math/Arts).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-23-implicit-bias/iat-480.webp 480w,/assets/img/2025-03-23-implicit-bias/iat-800.webp 800w,/assets/img/2025-03-23-implicit-bias/iat-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-23-implicit-bias/iat.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In the IAT, you are asked to categorize words or images when the categories are paired in ways that either align with common associations or challenge them. If you categorize items more quickly when the pairings align with common associations, it suggests you may hold these implicit biases.</p> <p>Similar to the traffic light example, the IAT reveals how difficult it is to override automatic associations.</p> <h3 id="why-implicit-bias-matters">Why Implicit Bias Matters</h3> <p>Implicit bias means that operates even under conditions of limited attention or cognitive load. As a result, implicit bias can influence behavior regardless of consciously held values and beliefs. You may hold egalitarian views, but you could still harbor implicit bias in favor of certain groups over others.</p> <p>Research demonstrates that implicit bias can relate to real-world outcomes, with researchers documenting its potential role in domains such as employment, healthcare, and criminal justice systems.</p> <h3 id="implicit-bias-in-large-language-models">Implicit Bias in Large Language Models</h3> <p>Large Language Models (LLMs) learn by absorbing patterns from vast amounts of human-generated text. Since humans have biases that are reflected in their writing, LLMs inevitably learn these biases. But beyond simply reproducing explicit biases present in their training data, an important question emerges: <em>Do these models develop something akin to the implicit biases we observe in humans?</em></p> <p>As a result, researchers have attempted to adapt methods from psychology to measure implicit bias in LLMs. For example, Bai et al. (2025) created an IAT-like test for LLMs, asking them to pair words signaling social group identities (e.g., “Julia” and “Ben”) with attribute words (e.g., “home”, “work”). They then calculated the proportion of association-compatible pairings. Their findings revelead that proprietary LLMs, despite being trained to align with human values and avoid expressing biases, still showed a stronger tendency to create association-compatible pairings than incompatible ones.</p> <p>However, there’s a fundamental issue with current approaches to measuring implicit bias in LLMs: they focus primarily on model outputs. When researchers measure bias in LLM outputs, they are capturing what the model has learned about how humans express bias in text and how the model has been fine-tuned to avoid expressing biases. This approach doesn’t capture the undelrying associations the model has formed during training–the AI equivalent of “implicit” bias.</p> <h3 id="reasoning-models-and-reasoning-tokens">Reasoning Models and Reasoning Tokens</h3> <p>Recently, there has been significant hype surrounding reasoning models in the AI community. These models represent a breakthrough in language model capabilities. Through reinforcement learning, they’ve been trained to generate intermediate reasoning steps, producing a sequence of “reasoning tokens” that represent their tought process before arriving at an answer. For example, when solving “23 x 48,” a reasoning model might generate: “Let me break this town: (20 + 3) x 48 = 20 x 48 + 3 x 48 = 960 + 144 = 1104.”</p> <p>This approach has dramatically improved performance on complex tasks like coding and arithmetic reasoning. Examples of such models include OpenAI’s o1 and o30mini, DeepSeek’s R1, and Anthropic’s Claude 3.7 Sonnet.</p> <p>Importantly, reasoning tokens provide a unique window into how models process information, paralleling response latencies used in the IAT. Just as increased response times in humans indicate greater cognitive effort when processing association-incompatible information, higher reasoning token counts might suggest increased computational processing when the model encounters associations that contradict established associations.</p> <h3 id="reasoning-models-exhibit-implicit-bias-like-patterns">Reasoning Models Exhibit Implicit Bias-like Patterns</h3> <p>Inspired by studies of implicit bias in humans, we investigated the amount of deliberation that reasoning models employ in processing association-compatible versus incompatible information. Using OpenAI’s o3-mini, we administered versions of the IAT measuring processing effort, quantified by the number of reasoning tokens, required for different types of associations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-23-implicit-bias/main-480.webp 480w,/assets/img/2025-03-23-implicit-bias/main-800.webp 800w,/assets/img/2025-03-23-implicit-bias/main-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-23-implicit-bias/main.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Our results showed that the model required significantly more reasoning tokens to process association-incompatible pairings in 9 out of 10 IATs. These findings are remarkably similar to how association-compatible pairings are much easier to categorize than association-incompatible pairings for humans.</p> <h3 id="why-this-matters">Why This Matters</h3> <p>The implications of implicit bias in reasoning models are significant, particularly as these systems are deployed in high-stakes decision contexts. Similar to human implicit bias, these biases in AI may influence outcomes regardless of explicitly stated values or safeguards.</p> <p>More specifically, reasoning models are being integrated into critical domains including healthcare diagnostics, hiring processes, legal risk assessments, and financial services. Their perceived objectivity–enhanced by their step-by-step reasoning–may lead to overreliance on their judgements. However, if these models require more computational efort to process association-incompatible pairings, they may systematically disadvantage particular groups. Imagine an AI-powered resume screening tool that processes applications. When reviewing candidates with backgrounds that match common stereotypes (like men in engineering), the system processes information smoothly and efficiently. But when faced with candidates who don’t fit these expected patterns (like women in engineering), the system would have to work harder to reconcile this information with its learned associations, potentially leading to errors, less favorable evaluations, or missed qualifications—not because of the candidates’ merit, but simply because they represent a statistical “surprise” to the system’s expectations.</p> <p>What makes this especially concerning is that these systems often present their conclusions with logical explanations that appear well-reasoned. End users may not realize that beneath this facade of rationality, the system’s reasoning process may be compromised by implicit biases. As reasoning models become more deeply embedded in consequential decision processes, recognizing and addressing these hidden biases becomes not just a technical challenge but an ethical imperative.</p> <h4 id="check-out-the-article-and-share-thoughts">Check Out the Article and Share Thoughts!</h4> <p>Lee, M. H. J., &amp; Lai, C. K. (2025). Implicit Bias-Like Patterns in Reasoning Models (No. arXiv:2503.11572). arXiv. https://doi.org/10.48550/arXiv.2503.11572</p> <h4 id="papers-cited-in-this-work">Papers Cited in this Work</h4> <p>Bai, X., Wang, A., Sucholutsky, I., &amp; Griffiths, T. L. (2025). Explicitly unbiased large language models still form biased associations. Proceedings of the National Academy of Sciences, 122(8), e2416228122. https://doi.org/10.1073/pnas.2416228122</p>]]></content><author><name></name></author><category term="research"/><category term="implicit-bias"/><summary type="html"><![CDATA[New pre-print on implicit bias in reasoning models]]></summary></entry><entry><title type="html">Homogeneity Bias as Differential Sampling Uncertainty</title><link href="https://lee-messi.github.io/blog/2025/sampling-uncertainty/" rel="alternate" type="text/html" title="Homogeneity Bias as Differential Sampling Uncertainty"/><published>2025-02-04T00:00:00+00:00</published><updated>2025-02-04T00:00:00+00:00</updated><id>https://lee-messi.github.io/blog/2025/sampling-uncertainty</id><content type="html" xml:base="https://lee-messi.github.io/blog/2025/sampling-uncertainty/"><![CDATA[<p>Recently, I came across an interesting paper investigating how regularizing attention weights could encourage BERT-like models to consider broader token contexts when constructing token-level representations thereby mitigating social biases (Attanasio et al., 2022). That’s quite technical, so let me walk you through what it means in simpler terms.</p> <h2 id="the-connection-between-attention-and-bias">The Connection between Attention and Bias</h2> <p>Think of attention in NLP models like a spotlight. When a model processes a word (token), it shines this spotlight on other words in the context to understand its meaning. For example, when processing the word “doctor,” traditional models might shine a very bright spotlight on words like “he” and a dimmer one on “she” - because that’s the pattern they learned from training data.</p> <p>The key idea behind attention regularization is to intentionally diffuse this spotlight–instead of having intense focus on specific gender-associated words, we want the model to distribute its attention more evenly. Here’s a way to think about it:</p> <p>In a normal setup, when processing “doctor”, the attention weights might look like:</p> <ul> <li>“he”: 0.7</li> <li>“she”: 0.2</li> <li>non-gendered pronouns: 0.1</li> </ul> <p>With regularization, we push for more balanced attention:</p> <ul> <li>“he”: 0.4</li> <li>“she”: 0.4</li> <li>non-gendered pronouns: 0.2</li> </ul> <p>By enforcing this more uniform distribution of attention, we reduce the model’s tendency to strongly associate certain professions with specific genders. It’s like telling the model: “Don’t jump to conclusions about gender when you’re thinking about occupations.”</p> <h2 id="how-it-relates-to-homogeneity-bias">How it Relates to Homogeneity Bias</h2> <p>This work on regularizing attention weights to mitigate social bias led me to consider homogeneity bias from a new angle. What if homogeneity bias in language models manifests from how these models sample tokens during inference? Specifically, when generating text about marginalized groups, the token sampling distribution might become more deterministic, focusing on a narrower set of possible next words.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-04-sampling-uncertainty/study_design-480.webp 480w,/assets/img/2025-02-04-sampling-uncertainty/study_design-800.webp 800w,/assets/img/2025-02-04-sampling-uncertainty/study_design-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-04-sampling-uncertainty/study_design.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We tested this idea using Vision-Language Models as our experimental setup. We asked these models to generate stories about images showing different racial and gender groups, then carefully tracked how the models made their word choices. To measure this scientifically, we used three different metrics: entropy, perplexity, and probability of differentiation. These metrics helped us understand if the model was more predictable when writing about certain groups versus others-similar to how a recommendation system might fall back on popular, safe suggestions when it has limited data about a user’s preferences.</p> <h2 id="results">Results</h2> <p>Our initial analysis with GPT-4 Turbo revealed a consistent pattern: when generating text, the model showed significantly lower uncertainty in its word choices when writing about Black Americans and women compared to White Americans and men. This pattern held true across all 50 token positions we analyzed.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-04-sampling-uncertainty/race-480.webp 480w,/assets/img/2025-02-04-sampling-uncertainty/race-800.webp 800w,/assets/img/2025-02-04-sampling-uncertainty/race-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-04-sampling-uncertainty/race.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To verify whether this pattern was unique to GPT-4 Turbo, we expanded our analysis to include three additional models: GPT-4o mini, Llama-3.2, and Ovis-1.6. Llama-3.2 exhibited similar patterns to GPT-4 Turbo, while GPT-4o mini and Ovis-1.6 either showed no significant differences or displayed opposite patterns in their token sampling behavior.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-04-sampling-uncertainty/race_position-480.webp 480w,/assets/img/2025-02-04-sampling-uncertainty/race_position-800.webp 800w,/assets/img/2025-02-04-sampling-uncertainty/race_position-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-04-sampling-uncertainty/race_position.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The visualizations above demonstrate consistently lower measures across token positions for Black Americans compared to White Americans, indicating more deterministic token sampling distributions for Black Americans.</p> <h2 id="the-significance-of-this-work">The Significance of this Work</h2> <p>Our approach offers a new way to understand homogeneity bias in language models by looking directly at how these models make word choices during text generation. Previous methods relied on using encoder models to analyze the final generated text, but these methods shared a fatal limitation: these encoder models might share the same biases as the models being studied, making it harder to isolate different sources of bias.</p> <p>By examining token sampling distributions during inference, we get a more direct window into how these models actually behave when generating text. Rather than looking at the end product, we’re observing the model’s decision-making process in real-time – specifically, how certain or uncertain it is when choosing words for different groups. This mechanistic approach helps us pinpoint where and how homogeneity bias manifests in the generation process, opening new possibilities for addressing these biases at their source.</p> <h3 id="references">References</h3> <p>Lee, M. H. J., &amp; Jeon, S. (2025). Homogeneity Bias as Differential Sampling Uncertainty in Language Models (arXiv:2501.19337). arXiv. https://doi.org/10.48550/arXiv.2501.19337</p> <p>Attanasio, G., Nozza, D., Hovy, D., &amp; Baralis, E. (2022). Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists (arXiv:2203.09192). arXiv. https://doi.org/10.48550/arXiv.2203.09192</p>]]></content><author><name></name></author><category term="research"/><category term="homogeneity-bias"/><summary type="html"><![CDATA[Introduction to a new paper]]></summary></entry><entry><title type="html">Homogeneity Bias in Large Language Models</title><link href="https://lee-messi.github.io/blog/2025/homogeneity-bias/" rel="alternate" type="text/html" title="Homogeneity Bias in Large Language Models"/><published>2025-01-08T00:00:00+00:00</published><updated>2025-01-08T00:00:00+00:00</updated><id>https://lee-messi.github.io/blog/2025/homogeneity-bias</id><content type="html" xml:base="https://lee-messi.github.io/blog/2025/homogeneity-bias/"><![CDATA[<p>This blog post is about my research at the intersection of Social Psychology and Artificial Intelligence. While AI might be familiar to many through recent developments like ChatGPT, the social psychology aspects might be less known. Let me walk you through how these fields connect, starting with a simple classroom example.</p> <h3 id="understanding-social-psychology-through-a-classroom">Understanding Social Psychology Through a Classroom</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-08-homogeneity-bias/classroom-480.webp 480w,/assets/img/2025-01-08-homogeneity-bias/classroom-800.webp 800w,/assets/img/2025-01-08-homogeneity-bias/classroom-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-08-homogeneity-bias/classroom.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Imagine walking into a classroom of 20 students on the first day of school. You’re surrounded by unfamiliar faces, feeling slightly awkward and anxious. As the semester progresses, you work on group projects and play together during recess. Gradually, that initial awkwardness fades as you form friendships. This transformation - how social interactions shape our thoughts, feelings, and behaviors - is what social psychologists study.</p> <h4 id="how-groups-form-and-divide">How Groups Form and Divide</h4> <p>In this classroom, students naturally split into groups. Let’s say one group loves sports and always plays outside during recess, while another prefers board games indoors. This division, though based simply on interests, can grow stronger over time.</p> <p>Think about being in the board game group. At first, you might casually invite someone from the sports group to join your game. After several rejections, you start assuming all sports group members wouldn’t be interested in board games and decide never to invite anyone again from the sports group. This assumption about an entire group is what we call a <em>stereotype</em>. The uncertainty you feel about asking them again is <em>prejudice</em>, and eventually choosing not to invite them is a form of <em>discrimination</em>.</p> <h4 id="seeing-other-groups-as-all-the-same">Seeing Other Groups as “All the Same”</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-08-homogeneity-bias/invitation-480.webp 480w,/assets/img/2025-01-08-homogeneity-bias/invitation-800.webp 800w,/assets/img/2025-01-08-homogeneity-bias/invitation-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-08-homogeneity-bias/invitation.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This classroom dynamic reveals a fascinating pattern: we tend to see members of other groups (outgroups) as more similar to each other than members of our own group (ingroup). As a board game enthusiast, you might think “all the sports kids are alike,” while seeing distinct personalities in your fellow board game players.</p> <h4 id="why-this-matters">Why This Matters</h4> <p>When we start seeing a group as “all the same,” something concerning happens. Research shows that this perception leads to more stereotyping, increased negative prejudice, and greater discrimination. Going back to our classroom example, once you start thinking of the sports group as all alike, you might be less likely to notice their individual interests or invite them to join different activities. You might even start making broader assumptions about what they like or don’t like, further widening the divide between groups.</p> <h3 id="ai-systems-learn-from-humans">AI Systems Learn From Humans</h3> <p>One of the most fascinating developments during my PhD has been the rise of Artificial Intelligence, particularly Large Language Models like ChatGPT. These models have shown an interesting tendency: they reproduce patterns of human stereotyping. For example, when writing stories about different groups, they often introduce the same gender stereotypes that humans have such as associating men with sports and politics, and women with family and emotions.</p> <p>This observation led me to a crucial question: if AI systems mirror human patterns of stereotyping, might they also show our tendency to see certain groups as “all the same”? Just as students in our classroom example might view the sports group as more similar to each other than they really are, would AI systems represent some groups as more similar than others?</p> <h4 id="measuring-ai-bias">Measuring AI Bias</h4> <p>To answer this question, we conducted an experiment. We asked ChatGPT to write various kinds of texts - stories, biographies, and self-introductions - about different racial/ethnic and gender groups. Then came the challenging part: how do we measure if AI sees certain groups as “more similar”?</p> <p>We approached this by examining how similar the AI’s writings were for each group. Think of it like this: if you asked someone to write multiple stories about different board game club members, and the stories were all very similar, you might conclude that this person sees board game club members as “all the same.” We did something similar with AI, using computational tools to measure how similar its writings were for different groups.</p> <h4 id="what-we-found">What We Found</h4> <p>The results were clear and concerning. Just like humans view certain groups as more similar to each other, AI systems showed the same pattern. When writing about racial/ethnic minorities and women, the AI’s stories were more similar to each other compared to its stories about White Americans and men. This happened even when we specifically asked for diverse, non-stereotypical stories.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-08-homogeneity-bias/bias-480.webp 480w,/assets/img/2025-01-08-homogeneity-bias/bias-800.webp 800w,/assets/img/2025-01-08-homogeneity-bias/bias-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-08-homogeneity-bias/bias.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Could this simply be because the AI was using similar topics or themes for certain groups? We tested this possibility too. While we did find that AI tended to write about specific themes (like adversity and hardship) more often for some groups, the similarity in writing went beyond just shared topics. Even when writing about the same theme, the AI’s descriptions were more similar for certain groups than others.</p> <h4 id="why-this-happens">Why This Happens</h4> <p>The explanation likely comes from how AI systems learn. These models are trained on massive amounts of human-written text from the internet and other sources. If human writers tend to represent certain groups in more homogeneous ways, the AI learns and reproduces these patterns. It’s similar to how students might form overly simplified views of other groups based on limited interactions - AI systems form their “views” based on the limited and potentially biased information in their training data.</p> <h4 id="looking-forward">Looking Forward</h4> <p>Our research has recently expanded to study whether these patterns appear when AI systems process facial images. This is particularly important as AI systems become more integrated into our daily lives, from content generation to decision support. Understanding these biases is the first step toward building fairer technology that respects the diversity of groups.</p> <p>If you’d like to learn more about this research, you can read our paper “<a href="https://dl.acm.org/doi/10.1145/3630106.3658975">Large Language Models Portray Socially Subordinate Groups as More Homogeneous, Consistent with a Bias Observed in Humans</a>” published in the ACM Conference on Fairness, Accountability, and Transparency 2024 (FAccT ‘24).</p> <p><em>Note: All visualizations included in the blog post were created using ChatGPT (GPT-4o)</em></p>]]></content><author><name></name></author><category term="research"/><category term="homogeneity-bias"/><summary type="html"><![CDATA[A short blog post about my research on homogeneity bias in Large Language Models]]></summary></entry></feed>